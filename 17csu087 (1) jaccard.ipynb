{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In \\[1\\]:\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    a = [0, 1, 2, 5, 6, 8, 9]\n",
    "    b = [0, 2, 3, 4, 5, 7, 9]\n",
    "\n",
    "In \\[2\\]:\n",
    "\n",
    "    def jaccard(list1, list2):\n",
    "        intersection = len(list(set(list1).intersection(list2)))\n",
    "        union = (len(list1) + len(list2)) - intersection\n",
    "        return float(intersection) / union\n",
    "\n",
    "    jaccard(a, b)\n",
    "\n",
    "Out\\[2\\]:\n",
    "\n",
    "    0.4\n",
    "\n",
    "In \\[5\\]:\n",
    "\n",
    "    import numpy\n",
    "\n",
    "    def levenshteinDistanceDP(token1, token2):\n",
    "        distances = numpy.zeros((len(token1) + 1, len(token2) + 1))\n",
    "\n",
    "        for t1 in range(len(token1) + 1):\n",
    "            distances[t1][0] = t1\n",
    "\n",
    "        for t2 in range(len(token2) + 1):\n",
    "            distances[0][t2] = t2\n",
    "            \n",
    "        printDistances(distances, len(token1), len(token2))\n",
    "        return 0\n",
    "\n",
    "    def printDistances(distances, token1Length, token2Length):\n",
    "        for t1 in range(token1Length + 1):\n",
    "            for t2 in range(token2Length + 1):\n",
    "                print(int(distances[t1][t2]), end=\" \")\n",
    "            print()\n",
    "            \n",
    "    levenshteinDistanceDP(\"kelm\", \"hello\")\n",
    "\n",
    "    0 1 2 3 4 5 \n",
    "    1 0 0 0 0 0 \n",
    "    2 0 0 0 0 0 \n",
    "    3 0 0 0 0 0 \n",
    "    4 0 0 0 0 0 \n",
    "\n",
    "Out\\[5\\]:\n",
    "\n",
    "    0\n",
    "\n",
    "In \\[6\\]:\n",
    "\n",
    "    doc_J = \"Julie loves me more than Linda loves me\"\n",
    "\n",
    "    doc_L = \"Jane likes me more than Julie loves me\"\n",
    "\n",
    "\n",
    "    documents = [doc_J, doc_L]\n",
    "\n",
    "In \\[9\\]:\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    import pandas as pd\n",
    "\n",
    "    count_vectorizer = CountVectorizer(stop_words='english')\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    sparse_matrix = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "\n",
    "    doc_term_matrix = sparse_matrix.todense()\n",
    "    df = pd.DataFrame(doc_term_matrix, \n",
    "                      columns=count_vectorizer.get_feature_names(), \n",
    "                      index=['doc_J', 'doc_L'])\n",
    "    df\n",
    "\n",
    "Out\\[9\\]:\n",
    "\n",
    "|        | jane | julie | likes | linda | loves | me  | more | than |\n",
    "|--------|------|-------|-------|-------|-------|-----|------|------|\n",
    "| doc\\_J | 0    | 1     | 0     | 1     | 2     | 2   | 1    | 1    |\n",
    "| doc\\_L | 1    | 1     | 1     | 0     | 1     | 2   | 1    | 1    |\n",
    "\n",
    "In \\[10\\]:\n",
    "\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    print(cosine_similarity(df, df))\n",
    "\n",
    "    [[1.         0.82158384]\n",
    "     [0.82158384 1.        ]]\n",
    "\n",
    "In \\[11\\]:\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    corpus = [\n",
    "        'This is the first document.',\n",
    "        'This document is the second document.',\n",
    "        'And this is the third one.',\n",
    "        'Is this the first document?', ]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    print(vectorizer.get_feature_names())\n",
    "    print(X.shape)\n",
    "\n",
    "    ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
    "    (4, 9)\n",
    "\n",
    "In \\[12\\]:\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    import seaborn as sns\n",
    "    import re\n",
    "\n",
    "    import pickle \n",
    "\n",
    "    import time\n",
    "\n",
    "\n",
    "    from nltk.tokenize import TweetTokenizer # doesn't split at apostrophes\n",
    "    import nltk\n",
    "    from nltk import Text\n",
    "    from nltk.tokenize import regexp_tokenize\n",
    "    from nltk.tokenize import word_tokenize  \n",
    "    from nltk.tokenize import sent_tokenize \n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.linear_model import LogisticRegression \n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "\n",
    "In \\[17\\]:\n",
    "\n",
    "    txt = [\"He is ::having a great Time, at the park time?\",\n",
    "           \"She, unlike most women, is a big player on the park's grass.\",\n",
    "           \"she can't be going\"]\n",
    "\n",
    "In \\[18\\]:\n",
    "\n",
    "    count_vec = CountVectorizer(stop_words=\"english\", analyzer='word',ngram_range=(1, 1), max_df=1.0, min_df=0.6, max_features=None)\n",
    "\n",
    "    count_train = count_vec.fit(txt)\n",
    "    bag_of_words = count_vec.transform(txt)\n",
    "\n",
    "    print(count_vec.get_feature_names())\n",
    "    print(\"\\nOnly 'park' becomes the vocabulary of the document term matrix (dtm) because it appears in 2 out of 3 documents, \\\n",
    "    meaning 0.66% of the time.\\\n",
    "          \\nThe rest of the words such as 'big' appear only in 1 out of 3 documents, meaning 0.33%. which is why they don't appear\")\n",
    "\n",
    "    ['park']\n",
    "\n",
    "    Only 'park' becomes the vocabulary of the document term matrix (dtm) because it appears in 2 out of 3 documents, meaning 0.66% of the time.      \n",
    "    The rest of the words such as 'big' appear only in 1 out of 3 documents, meaning 0.33%. which is why they don't appear\n",
    "\n",
    "In \\[19\\]:\n",
    "\n",
    "    count_vec = CountVectorizer(stop_words=\"english\", analyzer='word',ngram_range=(1, 1), max_df=0.50, min_df=1, max_features=None)\n",
    "\n",
    "    count_train = count_vec.fit(txt)\n",
    "    bag_of_words = count_vec.transform(txt)\n",
    "\n",
    "    print(count_vec.get_feature_names())\n",
    "    print(\"\\nOnly 'park' is ignored because it appears in 2 out of 3 documents, meaning 0.66% of the time.\")\n",
    "\n",
    "    ['big', 'going', 'grass', 'great', 'having', 'player', 'time', 'unlike', 'women']\n",
    "\n",
    "    Only 'park' is ignored because it appears in 2 out of 3 documents, meaning 0.66% of the time.\n",
    "\n",
    "In \\[21\\]:\n",
    "\n",
    "    import math\n",
    "    from textblob import TextBlob as tb\n",
    "\n",
    "    def tf(word, blob):\n",
    "        return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "    def n_containing(word, bloblist):\n",
    "        return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "    def idf(word, bloblist):\n",
    "        return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "    def tfidf(word, blob, bloblist):\n",
    "        return tf(word, blob) * idf(word, bloblist)\n",
    "\n",
    "In \\[22\\]:\n",
    "\n",
    "    document1 = tb(\"\"\"YouTube is an American online video-sharing platform headquartered in San Bruno, California.\n",
    "    The service, created in February 2005 by three former PayPal employees—Chad Hurley, Steve Chen, and Jawed Karim—was bought\n",
    "    by Google in November 2006 for US$1.65 billion and now operates as one of the company's subsidiaries. \n",
    "    YouTube is the second most-visited website after Google Search, according to Alexa Internet rankings.[7].\"\"\")\n",
    "\n",
    "    document2 = tb(\"\"\"YouTube is an American online video-sharing platform headquartered in San Bruno,\n",
    "    California. The service, created in February 2005 by three former PayPal employees—Chad Hurley, \n",
    "    Steve Chen, and Jawed Karim—was bought by Google in November 2006 for US$1.65 billion and now \n",
    "    operates as one of the company's subsidiaries. YouTube is the second most-visited website after \n",
    "    Google Search, according to Alexa Internet rankings.[7]\"\"\")\n",
    "\n",
    "    document3 = tb(\"\"\"All YouTube users can upload videos up to 15 minutes each in duration. \n",
    "    Users can verify their account, normally through a mobile phone, to gain the ability to \n",
    "    upload videos up to 12 hours in length, as well as produce live streams.[100][101] \n",
    "    When YouTube was launched in 2005, it was possible to upload longer videos, but a ten-minute limit\n",
    "    was introduced in March 2006 after YouTube found that the majority of videos exceeding this length were\n",
    "    unauthorized uploads of television shows and films.[102] The 10-minute limit was increased to 15 minutes in July 2010.[103\n",
    "    ] In the past, it was possible to upload videos longer than 12 hours. Videos can be at most 128 GB in size.[100] \n",
    "    Video captions are made using speech recognition technology when uploaded. Such captioning is usually not perfectly \n",
    "    accurate, so YouTube provides several options for manually entering the captions for greater accuracy.[104] YouTube \n",
    "    formerly offered a 'Community Captions' feature, where viewers could write and submit captions for public display upon\n",
    "    approval by the video uploader, but this was deprecated on September 28, 2020. (Captions that have already been added\n",
    "    using the feature will be retained..\"\"\")\n",
    "\n",
    "    bloblist = [document1, document2, document3]\n",
    "    for i, blob in enumerate(bloblist):\n",
    "        print(\"Top words in document {}\".format(i + 1))\n",
    "        scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "        sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        for word, score in sorted_words[:3]:\n",
    "            print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n",
    "\n",
    "    Top words in document 1\n",
    "        Word: an, TF-IDF: 0.0\n",
    "        Word: American, TF-IDF: 0.0\n",
    "        Word: online, TF-IDF: 0.0\n",
    "    Top words in document 2\n",
    "        Word: an, TF-IDF: 0.0\n",
    "        Word: American, TF-IDF: 0.0\n",
    "        Word: online, TF-IDF: 0.0\n",
    "    Top words in document 3\n",
    "        Word: In, TF-IDF: 0.01456\n",
    "        Word: videos, TF-IDF: 0.01248\n",
    "        Word: was, TF-IDF: 0.01248\n",
    "\n",
    "In \\[ \\]:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
